{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f80593",
   "metadata": {},
   "source": [
    "# SVM 與 MLP 的訓練過程、最佳化推導與程式實作（含 w^{*} = w + Δw）\n",
    "\n",
    "本筆記包含：\n",
    "1. **SVM 的訓練過程**：Soft-Margin（合頁損失）定義、次梯度、SGD 更新、（附）dual 觀念。\n",
    "2. **MLP 的訓練過程**：前向傳播、交叉熵損失、反向傳播（Backprop）梯度推導。\n",
    "3. **最佳的 W 推導直觀**：以 ∇L=0 與一階泰勒展開說明為何選擇 ΔW = -η ∇L。\n",
    "4. **在程式中實現** w^{*} = w + Δw：提供 NumPy 與 PyTorch（概念）對照與可跑的最小訓練範例（含 Loss 圖）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571333f2",
   "metadata": {},
   "source": [
    "## 1. SVM 的訓練過程（線性、Soft-Margin、二分類）\n",
    "\n",
    "模型與符號：資料 (x_i, y_i)，其中 x_i∈R^d，y_i∈{-1,+1}；分類面 f(x)=w^T x + b。\n",
    "\n",
    "Primal（標準軟間隔）：\n",
    "\n",
    "$$\n",
    "\\min_{w,b,\\xi} \\; \\tfrac{1}{2}\\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\quad \\text{s.t. } y_i(w^\\top x_i + b) \\ge 1 - \\xi_i,\\; \\xi_i \\ge 0.\n",
    "$$\n",
    "\n",
    "實作常用的等價形式（合頁損失 + L2）：\n",
    "\n",
    "$$\n",
    "L(w,b)=\\tfrac{\\lambda}{2}\\|w\\|^2 + \\tfrac{1}{n}\\sum_{i=1}^{n}\\max(0,\\,1-y_i(w^\\top x_i+b)).\n",
    "$$\n",
    "\n",
    "其中 λ ≈ 1/(Cn)。\n",
    "\n",
    "次梯度（以單一樣本 (x_i,y_i)）：令 m_i = y_i(w^T x_i+b)。\n",
    "\n",
    "- 若 m_i ≥ 1： ∂_w L = λ w，∂_b L = 0  \n",
    "- 若 m_i < 1： ∂_w L = λ w − y_i x_i，∂_b L = −y_i\n",
    "\n",
    "SGD 更新（即 w^{*} = w + Δw）：\n",
    "\n",
    "- 若 m_i ≥ 1： Δw = −η λ w，Δb = 0  \n",
    "- 若 m_i < 1： Δw = −η λ w + η y_i x_i，Δb = η y_i  \n",
    "\n",
    "更新則為 w ← w + Δw,  b ← b + Δb。\n",
    "\n",
    "（附）Dual 觀念：以拉格朗日乘子 α_i 可得對偶：\n",
    "\n",
    "$$\n",
    "\\max_\\alpha \\sum_{i=1}^n \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i\\alpha_j y_i y_j (x_i^\\top x_j)\n",
    "\\quad \\text{s.t. } 0\\le \\alpha_i \\le C,\\; \\sum_i \\alpha_i y_i = 0.\n",
    "$$\n",
    "\n",
    "最優時 w^{*}=\\sum_i \\alpha_i y_i x_i；b 可由 KKT 條件求得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM：Hinge + L2，簡易 SGD 訓練示範（NumPy）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_toy_linear_separable(n_per_class=100, d=2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mean_pos = np.ones(d)\n",
    "    mean_neg = -np.ones(d)\n",
    "    cov = np.eye(d) * 0.3\n",
    "    X_pos = rng.multivariate_normal(mean_pos, cov, size=n_per_class)\n",
    "    X_neg = rng.multivariate_normal(mean_neg, cov, size=n_per_class)\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(n_per_class), -np.ones(n_per_class)])\n",
    "    return X, y\n",
    "\n",
    "def svm_hinge_loss(w, b, X, y, lam):\n",
    "    margins = y * (X @ w + b)\n",
    "    hinge = np.maximum(0.0, 1.0 - margins)\n",
    "    return 0.5 * lam * (w @ w) + hinge.mean()\n",
    "\n",
    "def svm_sgd_train(X, y, epochs=50, eta=0.1, lam=1e-2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "    w = rng.normal(scale=0.1, size=d)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        idx = rng.permutation(n)\n",
    "        for i in idx:\n",
    "            xi, yi = X[i], y[i]\n",
    "            margin = yi * (xi @ w + b)\n",
    "            if margin >= 1.0:\n",
    "                delta_w = -eta * lam * w\n",
    "                delta_b = 0.0\n",
    "            else:\n",
    "                delta_w = -eta * lam * w + eta * yi * xi\n",
    "                delta_b = eta * yi\n",
    "            w = w + delta_w\n",
    "            b = b + delta_b\n",
    "        losses.append(svm_hinge_loss(w, b, X, y, lam))\n",
    "    return w, b, losses\n",
    "\n",
    "X, y = make_toy_linear_separable(n_per_class=120, d=2, seed=42)\n",
    "w, b, losses = svm_sgd_train(X, y, epochs=60, eta=0.1, lam=1e-2, seed=1)\n",
    "print('final loss:', float(losses[-1]))\n",
    "pred = np.sign(X @ w + b)\n",
    "acc = (pred == y).mean()\n",
    "print('train acc:', float(acc))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title('SVM training loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828784f",
   "metadata": {},
   "source": [
    "## 2. MLP（1 隱藏層）訓練過程與 Backprop 推導\n",
    "\n",
    "前向傳播（輸入 x∈R^d、隱藏 h、類別 C）：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{(1)} &= W_1 x + b_1,\\quad h=\\sigma(a^{(1)}) \\\\\n",
    "z &= W_2 h + b_2,\\quad \\hat{y}=\\operatorname{softmax}(z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "交叉熵損失（多類）：\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{C} y_{ik}\\,\\log \\hat{y}_{ik}.\n",
    "$$\n",
    "\n",
    "反向傳播（softmax + CE）：令 δ^{(2)}=ŷ−y。\n",
    "\n",
    "$$\n",
    "\\nabla_{W_2}L=\\delta^{(2)}h^\\top,\\quad \\nabla_{b_2}L=\\sum \\delta^{(2)}.\n",
    "$$\n",
    "$$\n",
    "\\delta^{(1)}=(W_2^\\top \\delta^{(2)})\\odot \\sigma'(a^{(1)}),\\quad\n",
    "\\nabla_{W_1}L=\\delta^{(1)}x^\\top,\\quad \\nabla_{b_1}L=\\sum \\delta^{(1)}.\n",
    "$$\n",
    "\n",
    "帶 L2 正則：在各 ∇_{W_ℓ}L 上再加 λ W_ℓ。\n",
    "\n",
    "參數更新：W_ℓ ← W_ℓ − η ∇_{W_ℓ}L,  b_ℓ ← b_ℓ − η ∇_{b_ℓ}L，即 w^{*}=w+Δw 且 Δw=−η∇L。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP（softmax + CE）訓練示範（NumPy）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(Z):\n",
    "    Z = Z - Z.max(axis=0, keepdims=True)\n",
    "    E = np.exp(Z)\n",
    "    return E / E.sum(axis=0, keepdims=True)\n",
    "\n",
    "def relu(A):\n",
    "    return np.maximum(0.0, A)\n",
    "\n",
    "def relu_grad(A):\n",
    "    return (A > 0).astype(A.dtype)\n",
    "\n",
    "def cross_entropy(probs, Y):\n",
    "    eps = 1e-12\n",
    "    return -np.mean(np.sum(Y * np.log(probs + eps), axis=0))\n",
    "\n",
    "def make_blobs_2class(n_per_class=150, d=2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mean1 = np.array([1.2, 0.8])\n",
    "    mean2 = np.array([-1.1, -0.7])\n",
    "    cov = np.eye(2) * 0.7\n",
    "    X1 = rng.multivariate_normal(mean1, cov, size=n_per_class)\n",
    "    X2 = rng.multivariate_normal(mean2, cov, size=n_per_class)\n",
    "    X = np.vstack([X1, X2])\n",
    "    y = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\n",
    "    return X, y\n",
    "\n",
    "def to_onehot(y, C):\n",
    "    Y = np.zeros((C, y.size))\n",
    "    Y[y, np.arange(y.size)] = 1.0\n",
    "    return Y\n",
    "\n",
    "def mlp_train(X, y, h=16, epochs=100, eta=0.1, lam=1e-3, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = X.shape[1]\n",
    "    C = int(y.max()) + 1\n",
    "    Xb = X.T\n",
    "    Y = to_onehot(y, C)\n",
    "    B = Xb.shape[1]\n",
    "\n",
    "    W1 = rng.normal(scale=0.1, size=(h, d)); b1 = np.zeros((h, 1))\n",
    "    W2 = rng.normal(scale=0.1, size=(C, h)); b2 = np.zeros((C, 1))\n",
    "\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        A1 = W1 @ Xb + b1\n",
    "        H = relu(A1)\n",
    "        Z = W2 @ H + b2\n",
    "        Yhat = softmax(Z)\n",
    "        loss = cross_entropy(Yhat, Y) + 0.5 * lam * (np.sum(W1*W1) + np.sum(W2*W2))\n",
    "        losses.append(loss)\n",
    "\n",
    "        delta2 = (Yhat - Y) / B\n",
    "        dW2 = delta2 @ H.T + lam * W2\n",
    "        db2 = delta2.sum(axis=1, keepdims=True)\n",
    "\n",
    "        delta1 = (W2.T @ delta2) * relu_grad(A1)\n",
    "        dW1 = delta1 @ Xb.T + lam * W1\n",
    "        db1 = delta1.sum(axis=1, keepdims=True)\n",
    "\n",
    "        W2 = W2 - eta * dW2\n",
    "        b2 = b2 - eta * db2\n",
    "        W1 = W1 - eta * dW1\n",
    "        b1 = b1 - eta * db1\n",
    "\n",
    "    A1 = W1 @ Xb + b1\n",
    "    H = relu(A1)\n",
    "    Z = W2 @ H + b2\n",
    "    Yhat = softmax(Z)\n",
    "    y_pred = np.argmax(Yhat, axis=0)\n",
    "    acc = (y_pred == y).mean()\n",
    "    return (W1, b1, W2, b2), losses, acc\n",
    "\n",
    "X, y = make_blobs_2class(n_per_class=160, d=2, seed=7)\n",
    "params, losses, acc = mlp_train(X, y, h=16, epochs=120, eta=0.15, lam=1e-3, seed=3)\n",
    "print('final loss:', float(losses[-1]))\n",
    "print('train acc:', float(acc))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title('MLP training loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2a3a9",
   "metadata": {},
   "source": [
    "## 3. 「最佳 W」的數學推導（直觀版）\n",
    "\n",
    "必要條件：可微時，極值點滿足 ∇_W L(W^{*})=0；凸問題（如 hinge+L2 的 w 部分）亦為全域最優條件。\n",
    "\n",
    "為何用 ΔW=−η ∇L？（一階泰勒）：\n",
    "\n",
    "$$\n",
    "L(W+ΔW) ≈ L(W) + (∇L(W))^T ΔW.\n",
    "$$\n",
    "\n",
    "沿 −∇L 方向取小步長 η>0，即 ΔW=−η ∇L(W)，可使近似下降。\n",
    "\n",
    "SVM：Dual 得 w^{*}=∑ α_i y_i x_i 並由 KKT 求 b；或在 Primal（合頁損失）上做次梯度下降到收斂。  \n",
    "MLP：非凸，以反向傳播計算 ∇L 並用梯度法（SGD/Adam 等）迭代尋找良好解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa9144",
   "metadata": {},
   "source": [
    "## 4. 在程式中實現 w^{*}=w+Δw（快速對照）\n",
    "\n",
    "**NumPy（已知梯度）**：\n",
    "\n",
    "```python\n",
    "w = w + (-eta * grad_w)  # Δw = -η ∇L\n",
    "```\n",
    "\n",
    "**在 SVM 更新裡**（對應前述分支）：\n",
    "\n",
    "```python\n",
    "if margin >= 1:\n",
    "    delta_w = -eta * lam * w\n",
    "else:\n",
    "    delta_w = -eta * lam * w + eta * y * x\n",
    "w = w + delta_w\n",
    "```\n",
    "\n",
    "**PyTorch（概念示意）**：\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p += -eta * p.grad   # w* = w + Δw\n",
    "        p.grad.zero_()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 額外示範：顯式計算 Δw 並更新\n",
    "import numpy as np\n",
    "\n",
    "w = np.array([1.0, -2.0, 0.5])\n",
    "grad_w = np.array([0.3, -0.2, 0.1])\n",
    "eta = 0.05\n",
    "delta_w = -eta * grad_w\n",
    "w_new = w + delta_w\n",
    "print('w:', w)\n",
    "print('grad_w:', grad_w)\n",
    "print('delta_w:', delta_w)\n",
    "print('w_new (w*):', w_new)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
