{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c27a1e",
   "metadata": {},
   "source": [
    "\n",
    "# SVM & MLP：訓練過程、損失函數與參數更新（對應白板說明）\n",
    "\n",
    "本筆記整理白板上的重點（MLP、Loss function、HW1：推導 SVM 與 MLP 的「訓練過程」、以及程式中如何實現 \\(W^\\*=W+\\Delta W\\)），提供對應的數學式與簡潔推導，並附上可執行的最小化程式骨架（不依賴資料集，只示範更新規則）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67572a0a",
   "metadata": {},
   "source": [
    "\n",
    "## 0. 共同核心：梯度下降與參數更新\n",
    "\n",
    "- 目標：最小化損失 \\(L(W)\\)（可含正則化）。  \n",
    "- 基本更新式（對應白板的 \\(W^\\* = W + \\Delta W\\)）：  \n",
    "\\[\n",
    "\\boxed{\\;\\Delta W = -\\eta \\nabla_W L(W)\\;,\\qquad W \\leftarrow W + \\Delta W = W - \\eta \\nabla_W L(W)\\;}\n",
    "\\]\n",
    "\n",
    "**參數意義**  \n",
    "\\(\\eta\\)：學習率（步長）；\\(\\nabla_W L\\)：對參數 \\(W\\) 的梯度；\\(L\\)：損失函數（見各模型）。\n",
    "\n",
    "> 小批次（mini-batch）時，\\(\\nabla_W L\\) 以該批資料的平均梯度近似；若含 L2 正則化 \\(\\frac{\\lambda}{2}\\|W\\|^2\\)，則梯度多出 \\(\\lambda W\\)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32625fde",
   "metadata": {},
   "source": [
    "\n",
    "## 1. SVM 的訓練過程（線性、soft-margin、二分類）\n",
    "\n",
    "- 分類面：\\(f(x) = w^\\top x + b\\)，標籤 \\(y_i \\in \\{-1,+1\\}\\)。  \n",
    "- 合頁損失（hinge）+ L2 正則化（常用於 SGD）：  \n",
    "\\[\n",
    "\\boxed{\\;L(w,b)=\\frac{\\lambda}{2}\\|w\\|^2+\\frac{1}{n}\\sum_{i=1}^{n}\\max\\bigl(0,\\;1-y_i(w^\\top x_i+b)\\bigr)\\;}\n",
    "\\]\n",
    "\n",
    "**次梯度（以單一樣本 \\( (x_i,y_i)\\) 說明）**：\n",
    "\\[\n",
    "\\partial_w L=\n",
    "\\begin{cases}\n",
    "\\lambda w & \\text{若 } y_i(w^\\top x_i+b)\\ge 1\\\\[2pt]\n",
    "\\lambda w - y_i x_i & \\text{若 } y_i(w^\\top x_i+b)< 1\n",
    "\\end{cases},\n",
    "\\qquad\n",
    "\\partial_b L=\n",
    "\\begin{cases}\n",
    "0 & \\text{若 } y_i(w^\\top x_i+b)\\ge 1\\\\\n",
    "- y_i & \\text{若 } y_i(w^\\top x_i+b)< 1\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "**SGD 更新（對應 \\(W^\\* = W + \\Delta W\\)）**：\n",
    "\\[\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\text{若 } y_i(w^\\top x_i+b)\\ge 1:&\\quad w\\leftarrow (1-\\eta\\lambda)w,\\; b\\leftarrow b\\\\\n",
    "\\text{否則 }&:\\quad w\\leftarrow (1-\\eta\\lambda)w+\\eta y_i x_i,\\; b\\leftarrow b+\\eta y_i\n",
    "\\end{aligned}}\n",
    "\\]\n",
    "\n",
    "> 意義：當樣本在「間隔內或被誤分」時（margin < 1）才用 \\(y_i x_i\\) 方向拉回決策面；否則只做權重衰減以實現 L2 正則化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 迷你版：線性 SVM 的單步 SGD 更新（示例）\n",
    "# x: (d,), y in {-1, +1}\n",
    "# w: (d,), b: scalar\n",
    "\n",
    "def svm_sgd_step(w, b, x, y, eta=1e-2, lam=1e-3):\n",
    "    margin = y * (w @ x + b)\n",
    "    if margin >= 1.0:\n",
    "        # 只做 L2 正則衰減\n",
    "        w = (1 - eta * lam) * w\n",
    "        # b 無變化\n",
    "    else:\n",
    "        # 合頁損失發生，推一把\n",
    "        w = (1 - eta * lam) * w + eta * y * x\n",
    "        b = b + eta * y\n",
    "    return w, b\n",
    "\n",
    "# 使用範例（隨機向量，僅示範 API）：\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    rng = np.random.default_rng(0)\n",
    "    w = rng.normal(size=5)\n",
    "    b = 0.0\n",
    "    x = rng.normal(size=5)\n",
    "    y = 1 if rng.random() > 0.5 else -1\n",
    "    w_new, b_new = svm_sgd_step(w, b, x, y)\n",
    "    print(\"w_new:\", w_new)\n",
    "    print(\"b_new:\", b_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f0efa",
   "metadata": {},
   "source": [
    "\n",
    "## 2. MLP（Multiple Layer Perceptron）訓練過程（以 1 隱藏層為例）\n",
    "\n",
    "### 前向傳播\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "a^{(1)} &= W_1 x + b_1,\\quad h=\\sigma(a^{(1)})\\\\\n",
    "z &= W_2 h + b_2,\\quad \\hat{y}=g(z)\n",
    "\\end{aligned}\n",
    "\\]\n",
    "- \\(\\sigma(\\cdot)\\)：隱藏層活化（ReLU、tanh…）  \n",
    "- \\(g(\\cdot)\\)：輸出層映射（分類常用 softmax；回歸多用恒等）\n",
    "\n",
    "### 常見損失\n",
    "- 多類分類（softmax + 交叉熵）：\n",
    "\\[\n",
    "\\boxed{L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k} y_{ik}\\,\\log \\hat{y}_{ik}}\n",
    "\\]\n",
    "- 回歸（均方誤差 MSE）：\n",
    "\\[\n",
    "\\boxed{L=\\frac{1}{2n}\\sum_{i=1}^{n}\\|\\hat{y}_i-y_i\\|^2}\n",
    "\\]\n",
    "\n",
    "### 反向傳播（以 softmax+CE 為例）\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\delta^{(2)} &= \\hat{y}-y \\quad (\\text{輸出層誤差})\\\\\n",
    "\\nabla_{W_2} L &= \\delta^{(2)} h^\\top,\\qquad \\nabla_{b_2} L=\\delta^{(2)}\\\\[4pt]\n",
    "\\delta^{(1)} &= \\bigl(W_2^\\top \\delta^{(2)}\\bigr)\\odot \\sigma'(a^{(1)})\\\\\n",
    "\\nabla_{W_1} L &= \\delta^{(1)} x^\\top,\\qquad \\nabla_{b_1} L=\\delta^{(1)}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "> 其中 \\(\\odot\\) 為逐元素乘，\\(\\sigma'\\) 為活化導數（ReLU 的導數為指示函數 \\(1_{a^{(1)}>0}\\)）。\n",
    "\n",
    "### 參數更新（對應 \\(W^\\*=W+\\Delta W\\)）\n",
    "\\[\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "W_2 &\\leftarrow W_2 - \\eta\\,\\nabla_{W_2}L,\\quad b_2 \\leftarrow b_2 - \\eta\\,\\nabla_{b_2}L\\\\\n",
    "W_1 &\\leftarrow W_1 - \\eta\\,\\nabla_{W_1}L,\\quad b_1 \\leftarrow b_1 - \\eta\\,\\nabla_{b_1}L\n",
    "\\end{aligned}}\n",
    "\\]\n",
    "若含 L2 正則化 \\(\\frac{\\lambda}{2}(\\|W_1\\|^2+\\|W_2\\|^2)\\)，則 \\(\\nabla_{W_\\ell}L\\) 額外加上 \\(\\lambda W_\\ell\\)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 迷你版：一隱藏層 MLP（softmax+CE）的單步反向傳播與參數更新骨架\n",
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - z.max(axis=0, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=0, keepdims=True)\n",
    "\n",
    "def relu(a):\n",
    "    return np.maximum(0.0, a)\n",
    "\n",
    "def relu_grad(a):\n",
    "    return (a > 0).astype(a.dtype)\n",
    "\n",
    "def cross_entropy(probs, y_onehot):\n",
    "    # probs: (C, B), y_onehot: (C, B)\n",
    "    # return scalar平均損失\n",
    "    eps = 1e-12\n",
    "    return -np.mean(np.sum(y_onehot * np.log(probs + eps), axis=0))\n",
    "\n",
    "def mlp_forward(W1, b1, W2, b2, x):\n",
    "    # x: (d, B)\n",
    "    a1 = W1 @ x + b1  # (h, B)\n",
    "    h = relu(a1)      # (h, B)\n",
    "    z = W2 @ h + b2   # (C, B)\n",
    "    yhat = softmax(z) # (C, B)\n",
    "    cache = (x, a1, h, z, yhat)\n",
    "    return yhat, cache\n",
    "\n",
    "def mlp_backward(W1, b1, W2, b2, cache, y_onehot, lam=0.0):\n",
    "    x, a1, h, z, yhat = cache\n",
    "    B = x.shape[1]\n",
    "\n",
    "    # 輸出層誤差\n",
    "    delta2 = (yhat - y_onehot) / B                    # (C, B)\n",
    "\n",
    "    # 梯度（含 L2 正則）\n",
    "    dW2 = delta2 @ h.T + lam * W2                     # (C, h)\n",
    "    db2 = delta2.sum(axis=1, keepdims=True)           # (C, 1)\n",
    "\n",
    "    delta1 = (W2.T @ delta2) * relu_grad(a1)          # (h, B)\n",
    "    dW1 = delta1 @ x.T + lam * W1                     # (h, d)\n",
    "    db1 = delta1.sum(axis=1, keepdims=True)           # (h, 1)\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def mlp_update(W1, b1, W2, b2, grads, eta=1e-2):\n",
    "    dW1, db1, dW2, db2 = grads\n",
    "    W1 = W1 - eta * dW1\n",
    "    b1 = b1 - eta * db1\n",
    "    W2 = W2 - eta * dW2\n",
    "    b2 = b2 - eta * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 使用範例（隨機資料，僅示範 API 與張量形狀）：\n",
    "if __name__ == \"__main__\":\n",
    "    rng = np.random.default_rng(0)\n",
    "    d, h, C, B = 8, 10, 3, 4  # input dim, hidden, classes, batch size\n",
    "    W1 = rng.normal(scale=0.1, size=(h, d)); b1 = np.zeros((h, 1))\n",
    "    W2 = rng.normal(scale=0.1, size=(C, h)); b2 = np.zeros((C, 1))\n",
    "\n",
    "    x = rng.normal(size=(d, B))                 # (d, B)\n",
    "    y_idx = rng.integers(0, C, size=(B,))       # (B,)\n",
    "    y_onehot = np.zeros((C, B)); y_onehot[y_idx, np.arange(B)] = 1.0\n",
    "\n",
    "    yhat, cache = mlp_forward(W1, b1, W2, b2, x)\n",
    "    loss = cross_entropy(yhat, y_onehot)\n",
    "    grads = mlp_backward(W1, b1, W2, b2, cache, y_onehot, lam=1e-3)\n",
    "    W1, b1, W2, b2 = mlp_update(W1, b1, W2, b2, grads, eta=1e-1)\n",
    "\n",
    "    print(\"loss (one step):\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c815e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Loss function 與「谷底圖」的連結\n",
    "\n",
    "沿著 \\(-\\eta \\nabla L\\) 方向，每步往低處前進；\\(\\eta\\) 太大可能震盪、太小則收斂慢。實務常用：\n",
    "- **學習率排程**（衰減 / 餘弦 / 週期性）  \n",
    "- **Momentum / Nesterov**、**Adam**、**RMSProp** 等自適應法  \n",
    "- **Early Stopping**、**權重衰減（L2）**、**Dropout** 等正則化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534c755",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 程式實作對照（Checklist）\n",
    "\n",
    "1. **前向**：依模型計算 \\(\\hat{y}\\) 與 \\(L\\)。  \n",
    "2. **反向**：依上式求所有 \\(\\nabla L\\)。  \n",
    "3. **更新**：依 \\(W \\leftarrow W - \\eta \\nabla L\\)（即白板的 \\(W^\\*=W+\\Delta W\\) 且 \\(\\Delta W=-\\eta\\nabla L\\)）。  \n",
    "4. **迭代**：直到收斂或達到訓練輪數。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
